{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d650ba29-e3cc-4af4-a5f2-3c6f8627d325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data quality \n",
    "execution layer (bronze silver gold)\n",
    "monitoring layer ( visialize those checks)\n",
    "\n",
    "# define dqx in yaml\n",
    "bronze: schema conformity, null checks, data type validation\n",
    "silver: uniqueness, referencial integrity, deduplication\n",
    "gold: business rules, thresholds, aggregations\n",
    "\n",
    "persist dq_results, include metadata \n",
    "\n",
    "# monitoring \n",
    "- dashboard\n",
    "- alerts \n",
    "- logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bbe1942-4bd2-4723-bf09-582cedaa9d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need to install dqx in our working session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a87531-999c-4c8e-aa82-f56e4ba005da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6f7d5f-d011-4b41-a90f-a43f1d37bd3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d685297-7336-4d9f-9146-123d91aa33e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion\n",
    "\n",
    "Here I am simulating raw data arrives to the bronze layer. This data comes from the Kaggle dataset [Netflix Data: Cleaning, Analysis and Visualization](https://www.kaggle.com/datasets/ariyoomotade/netflix-data-cleaning-analysis-and-visualization). The purpose is to explore dqx framework capabilities in data quality checks and monitoring through a data engineering project with Databricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d34e9610-a308-4fa4-b317-8bd8f1c54d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from databricks.labs.dqx.profiler.profiler import DQProfiler\n",
    "from databricks.labs.dqx.profiler.generator import DQGenerator\n",
    "from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e243d1-3f3a-4a46-9e6f-16e08025b32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from a csv file. In this case, the csv file is in the default volume, but data can come from different sources such as an SFTP that drops data to our container, or an S3 bucket, or a Kafka stream, etc.\n",
    "\n",
    "raw_data = spark.read.options(header=True, inferSchema=True).csv(\"/Volumes/workspace/default/files/DQX_demo_data/netflix_low_quality.csv\")\n",
    "\n",
    "# Displaying the data\n",
    "raw_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b17467-aa13-4a24-83f8-461e4410a56f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a client to interact with a workspace\n",
    "ws = WorkspaceClient()\n",
    "\n",
    "# Create an instance of DQProfiler \n",
    "profiler = DQProfiler(ws)\n",
    "# Profile the input data for basic statistics\n",
    "summary_stats, profiles = profiler.profile(raw_data)\n",
    "\n",
    "# Print the summary statistics and profiles\n",
    "print(yaml.safe_dump(summary_stats))\n",
    "print(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e3b4c5-64ce-487b-828a-5fdcc679f433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate the rules \n",
    "\n",
    "The function generate_dq_rules() creates a set of data quality rules based on the column profiles. These a recommendations we can leverage for our quality check process or we can create our own rules. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65b4692-e613-49e2-8d91-35c514e2e0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generator = DQGenerator(ws)\n",
    "checks = generator.generate_dq_rules(profiles)\n",
    "print(yaml.safe_dump(checks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60093d76-4a51-4516-bb86-631fe09c6c57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze quality checks\n",
    "\n",
    "It is a common practice to store data as raw as possible in the Bronze layer. In this case, the only quality check we are going to implement is checking nulls and correct format for show_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0ba5ac-6fc1-4964-b8c4-67515e2fc48d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_checks = [{'check': {'function': 'is_not_null_and_not_empty',\n",
    "   'arguments': {'column': 'show_id', 'trim_strings': True}},\n",
    "  'name': 'show_id_is_null_or_empty',\n",
    "  'criticality': 'error'},\n",
    "                 {'check': {'function': 'regex_match',\n",
    "   'arguments': {'column': 'show_id', 'regex': '^s\\\\d+$', 'negate': False}},\n",
    "  'name': 'invalid_show_id_format', 'criticality': 'error'}]\n",
    "  \n",
    "# Validate the data quality checks\n",
    "status = DQEngine.validate_checks(bronze_checks)\n",
    "print(status)\n",
    "# Can save these checks in our repository for future use and reusability of our rules by a larger team \n",
    "checks_file = f\"/Workspace/Users/nelsongilvargas@gmail.com/dqx_databricks_demo/utils/dqx_bronze_checks.yml\"\n",
    "dq_engine = DQEngine(ws)\n",
    "dq_engine.save_checks_in_workspace_file(bronze_checks, workspace_path=checks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf51d81-c5f3-4bc7-859f-67342c09505b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Dealing with invalid records\n",
    "## Option 1 \n",
    "One option given by dqx is splitting records in two sets: valid and invalid records. This is usefull to prevent low quality data in our persisted tables but at the same time keeping track of invalid data in a quarantine table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1bc9b8-c94e-4be5-9445-e0eb8a347232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check are already in memory but lets simulate that we are reading them from a file\n",
    "dq_engine = DQEngine(WorkspaceClient())\n",
    "checks_file = f\"/Workspace/Users/nelsongilvargas@gmail.com/dqx_databricks_demo/utils/dqx_bronze_checks.yml\"\n",
    "bronze_checks = dq_engine.load_checks_from_workspace_file(checks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3994d4ed-b8a3-44e8-80c1-22cb0e85368c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dq_engine = DQEngine(ws)\n",
    "\n",
    "valid_df, invalid_df = dq_engine.apply_checks_by_metadata_and_split(raw_data, bronze_checks)\n",
    "display(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c717bd4-32b9-4bef-a8f9-392b097b1123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here we can see the records where show_id is empty or has a different format than the one we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd3f6e3-5c47-4e16-ad3b-4834ab71ec09",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"_errors\":750},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756770034661}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(invalid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549d311e-424c-4116-a7c2-a26a1b5f06f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Option 2 \n",
    "dqx gives us the alternative to keep all the data together but adding two new columns '_errors' and '_warnings' which indicate the type of error or warning the record has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d226c45-a65b-4701-b430-c8eccad8c884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valid_and_quarantined_df =  dq_engine.apply_checks_by_metadata(raw_data, bronze_checks)\n",
    "display(valid_and_quarantined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78618bf8-ce6a-4206-8f8f-540cf5c1bacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Storing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16c3c97-a037-4bf8-aead-6039c3d5afef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "invalid_df = invalid_df.withColumn(\"write_timestamp\", lit(current_timestamp()))\n",
    "valid_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"dqx_demo_bronze_netflix_shows\")\n",
    "invalid_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"quarantine_dqx_demo_bronze_netflix_shows\")    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
