{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d650ba29-e3cc-4af4-a5f2-3c6f8627d325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Data quality \n",
    "execution layer (bronze silver gold)\n",
    "monitoring layer ( visialize those checks)\n",
    "\n",
    "# define dqx in yaml\n",
    "bronze: schema conformity, null checks, data type validation\n",
    "silver: uniqueness, referencial integrity, deduplication\n",
    "gold: business rules, thresholds, aggregations\n",
    "\n",
    "persist dq_results, include metadata \n",
    "\n",
    "# monitoring \n",
    "- dashboard\n",
    "- alerts \n",
    "- logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bbe1942-4bd2-4723-bf09-582cedaa9d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need to install dqx in our working session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33a87531-999c-4c8e-aa82-f56e4ba005da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-labs-dqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d685297-7336-4d9f-9146-123d91aa33e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Ingestion\n",
    "\n",
    "Here I am simulating raw data arrives to the bronze layer. This data comes from the Kaggle dataset [Netflix Data: Cleaning, Analysis and Visualization](https://www.kaggle.com/datasets/ariyoomotade/netflix-data-cleaning-analysis-and-visualization). The purpose is to explore dqx framework capabilities in data quality checks and monitoring through a data engineering project with Databricks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e243d1-3f3a-4a46-9e6f-16e08025b32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading data from a csv file. In this case, the csv file is in the default volume, but data can come from different sources such as an SFTP that drops data to our container, or an S3 bucket, or a Kafka stream, etc.\n",
    "\n",
    "raw_data = spark.read.options(header=True, inferSchema=True).csv(\"/Volumes/workspace/default/files/DQX_demo_data/netflix1.csv\")\n",
    "\n",
    "# Displaying the data\n",
    "raw_data.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
